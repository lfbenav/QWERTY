{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ce4f06",
   "metadata": {},
   "source": [
    "#  Agente Conversacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4b804",
   "metadata": {},
   "source": [
    "## Autores\n",
    "- ### Luis Benavides\n",
    "- ### Juan Jiménez\n",
    "- ### Alex Naranjo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d6f3c",
   "metadata": {},
   "source": [
    "## Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054654d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lfben\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.4)\n",
      "  from scipy.stats import fisher_exact\n",
      "c:\\Users\\lfben\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import langchain\n",
    "import pkgutil\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from duckduckgo_search import DDGS\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "import spacy\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "import unicodedata, re\n",
    "\n",
    "# NLTK 3.9+ requiere 'punkt_tab' además de 'punkt'. Descargamos ambos de forma silenciosa.\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "# Verificar GPU disponible\n",
    "if torch.cuda.is_available():\n",
    "\tprint(\"Usando GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c06f9",
   "metadata": {},
   "source": [
    "## 1. Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff1f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura de carpetas creada correctamente.\n"
     ]
    }
   ],
   "source": [
    "carpeta_data = \"data\"\n",
    "carpeta_pdf = \"data/pdf\"\n",
    "carpeta_processed = \"data/processed/clean_text\"\n",
    "carpeta_embeddings = \"data/embeddings\"\n",
    "carpeta_chunks_fijo = \"data/processed/chunks_fixed\"\n",
    "carpeta_chunks_sem = \"data/processed/chunks_semantic\"\n",
    "carpeta_db = \"data/databases\"\n",
    "\n",
    "agent_name = \"QWERTY\"\n",
    "\n",
    "# Crear carpetas necesarias\n",
    "for folder in [carpeta_data, carpeta_pdf, carpeta_processed, carpeta_embeddings, carpeta_chunks_sem, carpeta_chunks_fijo, carpeta_db]:\n",
    "\tos.makedirs(folder, exist_ok=True)\n",
    "print(\"Estructura de carpetas creada correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0260ff0",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento Textual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b19fcf",
   "metadata": {},
   "source": [
    "En esta etapa el objetivo es convertir los PDFs de los apuntes en texto limpio y homogéneo, eliminando caracteres extraños, tildes mal codificadas, saltos de línea innecesarios y normalizando todo a minúsculas.\n",
    "\n",
    "Esto permitirá que los embeddings sean más consistentes y que la base vectorial pueda recuperar mejor los fragmentos de texto relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001da86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Extracción ===\n",
    "def extraer_texto_pdf(ruta_pdf: str) -> str:\n",
    "\t\"\"\"\n",
    "\t\tExtrae texto del PDF usando pdfminer y corrige pseudoacentos ASCII\n",
    "\t\"\"\"\n",
    "\ttexto = extract_text(ruta_pdf) or \"\"\n",
    "\ttexto = unicodedata.normalize(\"NFC\", texto)\n",
    "\n",
    "\t# --- Correcciones comunes para PDFs en español mal codificados ---\n",
    "\tpatrones = {\n",
    "\t\tr\"˜n\": \"ñ\",\n",
    "\t\tr\"˜N\": \"Ñ\",\n",
    "\t\tr\" ´A\": \"Á\",\n",
    "\t\tr\" ´E\": \"É\",\n",
    "\t\tr\" ´I\": \"Í\",\n",
    "\t\tr\" ´O\": \"Ó\",\n",
    "\t\tr\" ´U\": \"Ú\",\n",
    "\t\tr\"´A\": \"Á\",\n",
    "\t\tr\"´E\": \"É\",\n",
    "\t\tr\"´I\": \"Í\",\n",
    "\t\tr\"´O\": \"Ó\",\n",
    "\t\tr\"´U\": \"Ú\",\n",
    "\t\tr\"´ı\": \"í\",\n",
    "\t\tr\"´a\": \"á\",\n",
    "\t\tr\"´e\": \"é\",\n",
    "\t\tr\"´i\": \"í\",\n",
    "\t\tr\"´o\": \"ó\",\n",
    "\t\tr\"´u\": \"ú\",\n",
    "\t\tr\"´n\": \"ñ\",\n",
    "\t\tr\"ı´\": \"í\",\n",
    "\t\tr\"a´\": \"á\",\n",
    "\t\tr\"e´\": \"é\",\n",
    "\t\tr\"i´\": \"í\",\n",
    "\t\tr\"o´\": \"ó\",\n",
    "\t\tr\"u´\": \"ú\",\n",
    "\t\tr\"A´\": \"Á\",\n",
    "\t\tr\"E´\": \"É\",\n",
    "\t\tr\"I´\": \"Í\",\n",
    "\t\tr\"O´\": \"Ó\",\n",
    "\t\tr\"U´\": \"Ú\",\n",
    "\t}\n",
    "\n",
    "\tfor k, v in patrones.items():\n",
    "\t\ttexto = re.sub(k, v, texto)\n",
    "\n",
    "\t# --- Limpieza básica adicional ---\n",
    "\ttexto = unicodedata.normalize(\"NFC\", texto)\n",
    "\ttexto = re.sub(r\"[ \\t]+\", \" \", texto)     # espacios dobles\n",
    "\ttexto = re.sub(r\"\\n{3,}\", \"\\n\\n\", texto)  # saltos excesivos\n",
    "\ttexto = texto.strip()\n",
    "\n",
    "\treturn texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto: str) -> str:\n",
    "\t\"\"\"\n",
    "\t\tLimpia y normaliza el texto extraído de PDFs.\n",
    "\t\t- Convierte a minúsculas.\n",
    "\t\t- Elimina tildes (á→a, ñ→n, etc.).\n",
    "\t\t- Quita caracteres especiales o símbolos raros.\n",
    "\t\t- Normaliza espacios.\n",
    "\t\"\"\"\n",
    "\n",
    "\tif not texto:\n",
    "\t\treturn \"\"\n",
    "\n",
    "\t# --- Normaliza Unicode (corrige combinaciones mal codificadas) ---\n",
    "\ttexto = unicodedata.normalize(\"NFKD\", texto)\n",
    "\n",
    "\t# --- Elimina los diacríticos (tildes, diéresis, etc.) ---\n",
    "\ttexto = \"\".join(c for c in texto if not unicodedata.combining(c))\n",
    "\n",
    "\t# --- Sustituye saltos de línea y espacios repetidos ---\n",
    "\ttexto = re.sub(r\"[\\r\\n\\t]+\", \" \", texto)\n",
    "\ttexto = re.sub(r\"\\s{2,}\", \" \", texto)\n",
    "\n",
    "\t# --- Elimina símbolos innecesarios ---\n",
    "\ttexto = re.sub(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ.,;:?!()¿¡+\\-*/=<>\\^% ]\", \"\", texto)\n",
    "\n",
    "\t# --- Minúsculas uniformes ---\n",
    "\ttexto = texto.lower().strip()\n",
    "\n",
    "\treturn texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce76d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Segmentación ===\n",
    "def segmentar_fijo(texto: str, tam_bloque: int = 800, solapamiento: int = 100) -> list:\n",
    "\t\"\"\"\n",
    "\t\tDivide el texto en bloques de longitud fija (en caracteres),\n",
    "\t\tcon un solapamiento configurable para preservar contexto.\n",
    "\t\"\"\"\n",
    "\tchunks = []\n",
    "\tinicio = 0\n",
    "\twhile inicio < len(texto):\n",
    "\t\tfin = inicio + tam_bloque\n",
    "\t\tfragmento = texto[inicio:fin]\n",
    "\t\tchunks.append(fragmento.strip())\n",
    "\t\tinicio += max(1, tam_bloque - solapamiento)\n",
    "\treturn chunks\n",
    "\n",
    "\n",
    "def segmentar_semantico(texto: str, tam_max: int = 800, solapamiento: int = 100) -> list:\n",
    "\t\"\"\"\n",
    "\t\tDivide el texto en fragmentos basados en oraciones (semántico).\n",
    "\t\t- Une oraciones hasta alcanzar tam_max.\n",
    "\t\t- Si una oración supera tam_max, se trocea con solapamiento.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef _partir_largo(texto: str, tam_bloque: int, solapamiento: int) -> list:\n",
    "\t\t\"\"\"\n",
    "\t\t\tParte un texto largo en ventanas de tamaño fijo con solapamiento.\n",
    "\t\t\"\"\"\n",
    "\t\tpartes = []\n",
    "\t\ti = 0\n",
    "\t\tstep = max(1, tam_bloque - solapamiento)\n",
    "\t\twhile i < len(texto):\n",
    "\t\t\tpartes.append(texto[i:i+tam_bloque])\n",
    "\t\t\ti += step\n",
    "\t\treturn [p.strip() for p in partes if p.strip()]\n",
    "\n",
    "\ttry:\n",
    "\t\t# Usar el modelo en español explícitamente para mejor segmentación de oraciones\n",
    "\t\toraciones = sent_tokenize(texto, language=\"spanish\")\n",
    "\texcept LookupError:\n",
    "\t\t# Intentar descargar recursos en caliente si faltan\n",
    "\t\tnltk.download(\"punkt\", quiet=True)\n",
    "\t\tnltk.download(\"punkt_tab\", quiet=True)\n",
    "\t\ttry:\n",
    "\t\t\toraciones = sent_tokenize(texto, language=\"spanish\")\n",
    "\t\texcept LookupError:\n",
    "\t\t\t# Fallback simple si NLTK sigue fallando\n",
    "\t\t\toraciones = re.split(r\"(?<=[\\.!?¡¿])\\s+\", texto)\n",
    "\n",
    "\tchunks, bloque = [], \"\"\n",
    "\tfor oracion in oraciones:\n",
    "\t\tif len(oracion) > tam_max:\n",
    "\n",
    "\t\t\t# Antes de añadir esta oración muy larga, vaciar el bloque actual\n",
    "\t\t\tif bloque:\n",
    "\t\t\t\tchunks.append(bloque.strip())\n",
    "\t\t\t\tbloque = \"\"\n",
    "\n",
    "\t\t\t# Trocear la oración larga en ventanas con solapamiento\n",
    "\t\t\ttrozos = _partir_largo(oracion, tam_bloque=tam_max, solapamiento=solapamiento)\n",
    "\t\t\tchunks.extend(trozos)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Acumular oraciones normales hasta tam_max\n",
    "\t\tif len(bloque) + len(oracion) + 1 <= tam_max:\n",
    "\t\t\tbloque = (bloque + \" \" + oracion).strip()\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tif bloque:\n",
    "\t\t\t\tchunks.append(bloque.strip())\n",
    "\t\t\tbloque = oracion\n",
    "\n",
    "\tif bloque:\n",
    "\t\tchunks.append(bloque.strip())\n",
    "\n",
    "\treturn chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e664637",
   "metadata": {},
   "source": [
    "### Aplicar el Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a202b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Procesamiento general de PDFs ===\n",
    "def preprocesar_archivos():\n",
    "\tmetadata_records = []\n",
    "\n",
    "\tfor archivo in os.listdir(carpeta_pdf):\n",
    "\t\tif archivo.endswith(\".pdf\"):\n",
    "\t\t\truta_pdf = os.path.join(carpeta_pdf, archivo)\n",
    "\t\t\tprint(f\"Procesando: {archivo}\")\n",
    "\n",
    "\t\t\t# --- Extraer metadata desde el nombre ---\n",
    "\t\t\tpartes = archivo.replace(\".pdf\", \"\").split(\"_\")\n",
    "\t\t\tsemana = partes[0] if len(partes) > 0 else \"N/A\"\n",
    "\t\t\tfecha = partes[3] if len(partes) > 3 else \"00000000\"\n",
    "\t\t\tversion = partes[4] if len(partes) > 4 else \"1\"\n",
    "\n",
    "\t\t\t# --- Extraer y limpiar texto ---\n",
    "\t\t\ttexto = extraer_texto_pdf(ruta_pdf)\n",
    "\t\t\ttexto_limpio = limpiar_texto(texto)\n",
    "\n",
    "\t\t\t# --- Guardar texto limpio completo ---\n",
    "\t\t\tnombre_txt = archivo.replace(\".pdf\", \".txt\")\n",
    "\t\t\truta_txt = os.path.join(carpeta_processed, nombre_txt)\n",
    "\t\t\twith open(ruta_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\tf.write(texto_limpio)\n",
    "\n",
    "\t\t\t# --- A. Segmentar Fijo ---\n",
    "\t\t\tchunks_fijo = segmentar_fijo(texto_limpio, tam_bloque=800, solapamiento=100)\n",
    "\t\t\tfor i, ch in enumerate(chunks_fijo):\n",
    "\t\t\t\truta_chunk = os.path.join(carpeta_chunks_fijo, f\"{nombre_txt}_chunk_{i}.txt\")\n",
    "\t\t\t\twith open(ruta_chunk, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\t\tf.write(ch)\n",
    "\n",
    "\t\t\t# --- B. Segmentar Semántico ---\n",
    "\t\t\tchunks_sem = segmentar_semantico(texto_limpio, tam_max=800, solapamiento=100)\n",
    "\t\t\tfor i, ch in enumerate(chunks_sem):\n",
    "\t\t\t\truta_chunk = os.path.join(carpeta_chunks_sem, f\"{nombre_txt}_chunk_{i}.txt\")\n",
    "\t\t\t\twith open(ruta_chunk, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\t\tf.write(ch)\n",
    "\n",
    "\t\t\t# --- Registrar metadata ---\n",
    "\t\t\tmetadata_records.append({\n",
    "\t\t\t\t\"archivo_pdf\": archivo,\n",
    "\t\t\t\t\"archivo_txt\": nombre_txt,\n",
    "\t\t\t\t\"semana\": semana,\n",
    "\t\t\t\t\"fecha\": fecha,\n",
    "\t\t\t\t\"version\": version,\n",
    "\t\t\t\t\"ruta_txt\": ruta_txt,\n",
    "\t\t\t\t\"num_chunks_fijo\": len(chunks_fijo),\n",
    "\t\t\t\t\"num_chunks_sem\": len(chunks_sem)\n",
    "\t\t\t})\n",
    "\n",
    "\t\t\tprint(f\"Texto limpio guardado y segmentado ({len(chunks_fijo)} + {len(chunks_sem)} chunks)\")\n",
    "\n",
    "\t# === Guardar metadata general ===\n",
    "\tmetadata_df = pd.DataFrame(metadata_records)\n",
    "\tmetadata_path = f\"{carpeta_data}/metadata.csv\"\n",
    "\tmetadata_df.to_csv(metadata_path, index=False, encoding=\"utf-8\")\n",
    "\tprint(f\"\\nMetadata registrada en: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 10_SEMANA_AI_20251007_1-222887296.pdf\n",
      "Texto limpio guardado y segmentado (11 + 11 chunks)\n",
      "Procesando: 10_SEMANA_AI_20251007_1.pdf\n",
      "Texto limpio guardado y segmentado (19 + 20 chunks)\n",
      "Procesando: 10_SEMANA_AI_20251009_1.pdf\n",
      "Texto limpio guardado y segmentado (11 + 11 chunks)\n",
      "Procesando: 11_Semana_AI_20251014_1.pdf\n",
      "Texto limpio guardado y segmentado (20 + 19 chunks)\n",
      "Procesando: 11_Semana_AI_20251014_2.pdf\n",
      "Texto limpio guardado y segmentado (24 + 24 chunks)\n",
      "Procesando: 11_Semana_AI_20251014_3.pdf\n",
      "Texto limpio guardado y segmentado (14 + 13 chunks)\n",
      "Procesando: 11_SEMANA_AI_20251016_2.pdf\n",
      "Texto limpio guardado y segmentado (12 + 12 chunks)\n",
      "Procesando: 11_Semana_AI_20251016_4.pdf\n",
      "Texto limpio guardado y segmentado (18 + 17 chunks)\n",
      "Procesando: 12_SEMANA_AI_20251021_1.pdf\n",
      "Texto limpio guardado y segmentado (11 + 11 chunks)\n",
      "Procesando: 12_Semana_AI_20251021_2.pdf\n",
      "Texto limpio guardado y segmentado (23 + 23 chunks)\n",
      "Procesando: 12_SEMANA_AI_20251021_3.pdf\n",
      "Texto limpio guardado y segmentado (9 + 9 chunks)\n",
      "Procesando: 12_SEMANA_AI_20251021_4.pdf\n",
      "Texto limpio guardado y segmentado (36 + 37 chunks)\n",
      "Procesando: 12_SEMANA_AI_20251023_1.pdf\n",
      "Texto limpio guardado y segmentado (10 + 10 chunks)\n",
      "Procesando: 12_Semana_AI_20251023_3.pdf\n",
      "Texto limpio guardado y segmentado (7 + 7 chunks)\n",
      "Procesando: 12_SEMANA_AL_20251023_2.pdf\n",
      "Texto limpio guardado y segmentado (19 + 20 chunks)\n",
      "Procesando: 1_SEMANA_AI_20250807_1.pdf\n",
      "Texto limpio guardado y segmentado (14 + 14 chunks)\n",
      "Procesando: 1_Semana_AI_20250807_2.pdf\n",
      "Texto limpio guardado y segmentado (13 + 12 chunks)\n",
      "Procesando: 2_SEMANA_AI_20250812_1.pdf\n",
      "Texto limpio guardado y segmentado (21 + 21 chunks)\n",
      "Procesando: 2_Semana_AI_20250812_3.pdf\n",
      "Texto limpio guardado y segmentado (19 + 20 chunks)\n",
      "Procesando: 2_Semana_AI_20250814_1.pdf\n",
      "Texto limpio guardado y segmentado (27 + 27 chunks)\n",
      "Procesando: 2_Semana_AI_20250814_2.pdf\n",
      "Texto limpio guardado y segmentado (23 + 26 chunks)\n",
      "Procesando: 3_Semana_AI_20250819_1.pdf\n",
      "Texto limpio guardado y segmentado (15 + 15 chunks)\n",
      "Procesando: 3_Semana_AI_20250819_2.pdf\n",
      "Texto limpio guardado y segmentado (13 + 12 chunks)\n",
      "Procesando: 3_Semana_AI_20250821_1.pdf\n",
      "Texto limpio guardado y segmentado (14 + 14 chunks)\n",
      "Procesando: 4_SEMANA_AI_20250826_1.pdf\n",
      "Texto limpio guardado y segmentado (8 + 8 chunks)\n",
      "Procesando: 4_SEMANA_AI_20250826_2.pdf\n",
      "Texto limpio guardado y segmentado (10 + 9 chunks)\n",
      "Procesando: 4_Semana_AI_20250828_1.pdf\n",
      "Texto limpio guardado y segmentado (18 + 17 chunks)\n",
      "Procesando: 4_Semana_AI_20250828_2.pdf\n",
      "Texto limpio guardado y segmentado (12 + 12 chunks)\n",
      "Procesando: 5_Semana_AI_20250902_1.pdf\n",
      "Texto limpio guardado y segmentado (15 + 15 chunks)\n",
      "Procesando: 5_Semana_AI_20250902_2.pdf\n",
      "Texto limpio guardado y segmentado (8 + 9 chunks)\n",
      "Procesando: 5_Semana_AI_20250904_1.pdf\n",
      "Texto limpio guardado y segmentado (20 + 19 chunks)\n",
      "Procesando: 5_Semana_AI_20250904_2.pdf\n",
      "Texto limpio guardado y segmentado (13 + 13 chunks)\n",
      "Procesando: 6_Semana_AI_20250909_1.pdf\n",
      "Texto limpio guardado y segmentado (9 + 9 chunks)\n",
      "Procesando: 6_Semana_AI_20250909_2-220676337.pdf\n",
      "Texto limpio guardado y segmentado (10 + 9 chunks)\n",
      "Procesando: 6_Semana_AI_20250911_1.pdf\n",
      "Texto limpio guardado y segmentado (14 + 16 chunks)\n",
      "Procesando: 6_Semana_AI_20250911_2.pdf\n",
      "Texto limpio guardado y segmentado (15 + 14 chunks)\n",
      "Procesando: 7_Semana_AI_20250916_1.pdf\n",
      "Texto limpio guardado y segmentado (22 + 22 chunks)\n",
      "Procesando: 7_Semana_AI_20250916_2.pdf\n",
      "Texto limpio guardado y segmentado (18 + 17 chunks)\n",
      "Procesando: 7_Semana_AI_20250918_1.pdf\n",
      "Texto limpio guardado y segmentado (10 + 11 chunks)\n",
      "Procesando: 7_Semana_AI_20250918_2.pdf\n",
      "Texto limpio guardado y segmentado (12 + 12 chunks)\n",
      "Procesando: 8_Semana_AI_20250923_1.pdf\n",
      "Texto limpio guardado y segmentado (14 + 13 chunks)\n",
      "Procesando: 8_Semana_AI_20250923_2.pdf\n",
      "Texto limpio guardado y segmentado (11 + 12 chunks)\n",
      "Procesando: 8_Semana_AI_20250925_1.pdf\n",
      "Texto limpio guardado y segmentado (6 + 6 chunks)\n",
      "Procesando: 8_Semana_AI_20250925_2.pdf\n",
      "Texto limpio guardado y segmentado (22 + 24 chunks)\n",
      "Procesando: 9_SEMANA_AI_20251002_1.pdf\n",
      "Texto limpio guardado y segmentado (28 + 28 chunks)\n",
      "Procesando: 9_Semana_AI_20251002_2.pdf\n",
      "Texto limpio guardado y segmentado (22 + 22 chunks)\n",
      "\n",
      "Metadata registrada en: data/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# # === Aplicar el preprocesamiento ===\n",
    "# preprocesar_archivos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b269a",
   "metadata": {},
   "source": [
    "## 3. Tokenización y Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19a3f9",
   "metadata": {},
   "source": [
    "El objetivo de esta etapa es transformar cada documento procesado en una representación numérica (vector) que capture su significado semántico.\n",
    "Estos embeddings permitirán que el agente RAG busque los fragmentos más relevantes según las preguntas del usuario.\n",
    "  - Costo bajo y buena precisión.\n",
    "  - Compatible con langchain y el cliente oficial de OpenAI.\n",
    "- Entrada: textos limpios (uno por cada PDF procesado).\n",
    "- Salida: vectores almacenados junto con su metadata en un archivo CSV.\n",
    "\n",
    "En esta etapa se generan embeddings para los dos conjuntos de fragmentos (segmentación fija y segmentación semántica), utilizando el modelo local de Hugging Face ```intfloat/multilingual-e5-base```.\n",
    "\n",
    "Este modelo convierte texto en vectores numéricos (768 dims) que capturan su significado semántico, permitiendo búsquedas por similitud. No requiere API ni conexión externa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e5c5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:57:59.974 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:57:59.976 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2025-11-05 18:58:04.612 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:04.613 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:04.614 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# === Configuración del modelo ===\n",
    "EMBED_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "if \"model\" not in st.session_state:\n",
    "    st.session_state.model = SentenceTransformer(EMBED_MODEL_NAME, device=device)\n",
    "model = st.session_state.model\n",
    "\n",
    "# === Función para procesar fragmentos ===\n",
    "def procesar_directorio_chunks(carpeta_chunks: str, tipo: str, carpeta_salida: str) -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\t\tGenera embeddings para todos los fragmentos de texto en una carpeta.\n",
    "\t\t'tipo' indica la segmentación: 'fixed' o 'semantic'.\n",
    "\t\"\"\"\n",
    "\tregistros = []\n",
    "\tif not os.path.isdir(carpeta_chunks):\n",
    "\t\tprint(f\"[Aviso] Carpeta no encontrada: {carpeta_chunks}\")\n",
    "\t\treturn pd.DataFrame([])\n",
    "\n",
    "\tarchivos = sorted(os.listdir(carpeta_chunks))\n",
    "\tprint(f\"\\nGenerando embeddings ({tipo}): {len(archivos)} fragmentos encontrados.\")\n",
    "\n",
    "\ttextos, rutas = [], []\n",
    "\n",
    "\t# --- Recolección de fragmentos ---\n",
    "\tfor archivo in archivos:\n",
    "\t\truta = os.path.join(carpeta_chunks, archivo)\n",
    "\t\twith open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\ttexto = f.read().strip()\n",
    "\t\tif texto:\n",
    "\t\t\ttextos.append(texto)\n",
    "\t\t\trutas.append(ruta)\n",
    "\n",
    "\tif not textos:\n",
    "\t\tprint(f\"[Aviso] No se encontraron textos válidos en {carpeta_chunks}\")\n",
    "\t\treturn pd.DataFrame([])\n",
    "\n",
    "\t# --- Generación por lotes ---\n",
    "\tembeddings = model.encode(\n",
    "\t\ttextos,\n",
    "\t\tshow_progress_bar=True,\n",
    "\t\tbatch_size=16,\n",
    "\t\tconvert_to_numpy=True,\n",
    "\t\tnormalize_embeddings=True\n",
    "\t)\n",
    "\n",
    "\t# --- Construcción del DataFrame ---\n",
    "\tfor i, (emb, ruta) in enumerate(zip(embeddings, rutas)):\n",
    "\t\tregistros.append({\n",
    "\t\t\t\"fragmento_id\": f\"{tipo}_{i}\",\n",
    "\t\t\t\"ruta_fragmento\": ruta,\n",
    "\t\t\t\"tipo_segmentacion\": tipo,\n",
    "\t\t\t\"embedding\": emb.tolist()\n",
    "\t\t})\n",
    "\n",
    "\t# --- Guardado del DataFrame ---\n",
    "\tdf = pd.DataFrame(registros)\n",
    "\truta_final = f\"{carpeta_salida}/{tipo}.csv\"\n",
    "\tdf.to_csv(ruta_final, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\tprint(f\"Embeddings ({tipo}) guardados en {ruta_final}\")\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generando embeddings (fixed): 720 fragmentos encontrados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 45/45 [00:08<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings (fixed) guardados en data/embeddings/fixed.csv\n",
      "\n",
      "Generando embeddings (semantic): 722 fragmentos encontrados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 46/46 [00:07<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings (semantic) guardados en data/embeddings/semantic.csv\n",
      "\n",
      "Resumen final:\n",
      "Embeddings fijos generados: 720\n",
      "Embeddings semánticos generados: 722\n"
     ]
    }
   ],
   "source": [
    "# # === Procesar ambas segmentaciones ===\n",
    "# df_fixed = procesar_directorio_chunks(carpeta_chunks_fijo, \"fixed\", carpeta_embeddings)\n",
    "# df_sem   = procesar_directorio_chunks(carpeta_chunks_sem, \"semantic\", carpeta_embeddings)\n",
    "\n",
    "# print(\"\\nResumen final:\")\n",
    "# print(f\"Embeddings fijos generados: {len(df_fixed)}\")\n",
    "# print(f\"Embeddings semánticos generados: {len(df_sem)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad83b652",
   "metadata": {},
   "source": [
    "## 4. Herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0844d0b",
   "metadata": {},
   "source": [
    "### Construcción y Carga de Bases Vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15646d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:58:26.580 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\lfben\\AppData\\Local\\Temp\\ipykernel_30244\\1962929923.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  st.session_state.embedding_fn = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
      "2025-11-05 18:58:29.783 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:29.784 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:29.784 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# === Embeddings function ===\n",
    "if \"embedding_fn\" not in st.session_state:\n",
    "    st.session_state.embedding_fn = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "embedding_fn = st.session_state.embedding_fn\n",
    "\n",
    "# === Construir / cargar VectorStores (Chroma) por segmentación ===\n",
    "def _build_or_load_chroma(persist_dir: str, csv_path: str):\n",
    "\t\"\"\"\n",
    "\t\tCarga (si existe) o crea (si no) una base vectorial Chroma\n",
    "\t\treutilizando los embeddings precomputados guardados en CSV.\n",
    "\t\tCompatible con langchain==1.0.3\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Intentar cargar base existente\n",
    "\ttry:\n",
    "\t\tdb = Chroma(persist_directory=persist_dir, embedding_function=embedding_fn)\n",
    "\t\tcount = db._collection.count()\n",
    "\t\tif count > 0:\n",
    "\t\t\tprint(f\"Chroma cargada desde {persist_dir} ({count} vectores)\")\n",
    "\t\t\treturn db\n",
    "\texcept Exception:\n",
    "\t\tpass\n",
    "\n",
    "\t# Crear desde CSV\n",
    "\tif not os.path.exists(csv_path):\n",
    "\t\traise FileNotFoundError(f\"No existe el CSV: {csv_path}\")\n",
    "\n",
    "\tprint(f\"Construyendo Chroma desde embeddings: {csv_path}\")\n",
    "\tdf = pd.read_csv(csv_path)\n",
    "\tdf[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(json.loads(x)))\n",
    "\tembeddings = np.vstack(df[\"embedding\"].to_numpy())\n",
    "\t\n",
    "\ttextos = []\n",
    "\tmetadatas = []\n",
    "\tids = []\n",
    "\tfor _, row in df.iterrows():\n",
    "\t\truta = row[\"ruta_fragmento\"]\n",
    "\t\ttry:\n",
    "\t\t\twith open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\ttexto = f.read().strip()\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\ttexto = \"[Fragmento no encontrado]\"\n",
    "\t\ttextos.append(texto)\n",
    "\t\tmetadatas.append({\n",
    "\t\t\t\"ruta_fragmento\": ruta,\n",
    "\t\t\t\"tipo_segmentacion\": row[\"tipo_segmentacion\"],\n",
    "\t\t\t\"fragmento_id\": row[\"fragmento_id\"],\n",
    "\t\t})\n",
    "\t\tids.append(row[\"fragmento_id\"])\n",
    "\n",
    "\tdb = Chroma(\n",
    "\t\tpersist_directory=persist_dir,\n",
    "\t\tembedding_function=embedding_fn\n",
    "\t)\n",
    "\n",
    "\tdb._collection.add(\n",
    "\t\tids=ids,\n",
    "\t\tdocuments=textos,\n",
    "\t\tembeddings=embeddings.astype(np.float32).tolist(),\n",
    "\t\tmetadatas=metadatas,\n",
    "\t)\n",
    "\n",
    "\tdb.persist()\n",
    "\tprint(f\"Chroma creada en {persist_dir} (docs: {len(df)})\")\n",
    "\treturn db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93246372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:58:29.795 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\lfben\\AppData\\Local\\Temp\\ipykernel_30244\\1962929923.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=persist_dir, embedding_function=embedding_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo Chroma desde embeddings: data/embeddings/fixed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lfben\\AppData\\Local\\Temp\\ipykernel_30244\\1962929923.py:63: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n",
      "2025-11-05 18:58:31.624 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:31.625 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma creada en data/databases/chroma_fixed (docs: 720)\n",
      "Construyendo Chroma desde embeddings: data/embeddings/semantic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:58:32.883 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:32.884 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:32.884 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:32.885 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma creada en data/databases/chroma_semantic (docs: 722)\n"
     ]
    }
   ],
   "source": [
    "# === Crear/cargar ambas colecciones ===\n",
    "if \"chroma_fixed\" not in st.session_state or \"chroma_sem\" not in st.session_state:\n",
    "    st.session_state.chroma_fixed = _build_or_load_chroma(f\"{carpeta_db}/chroma_fixed\", f\"{carpeta_embeddings}/fixed.csv\")\n",
    "    st.session_state.chroma_sem = _build_or_load_chroma(f\"{carpeta_db}/chroma_semantic\", f\"{carpeta_embeddings}/semantic.csv\")\n",
    "\n",
    "chroma_fixed = st.session_state.chroma_fixed\n",
    "chroma_sem = st.session_state.chroma_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39dc2bb",
   "metadata": {},
   "source": [
    "### Herramienta RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e1227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_chunk_0(nombre_archivo: str, carpeta_texto: str = carpeta_chunks_sem) -> str:\n",
    "\t\"\"\"\n",
    "\t\tObtiene el contenido del primer chunk (chunk_0.txt) del mismo documento base.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Identificar el nombre base (antes del sufijo _chunk_X.txt)\n",
    "\tbase_name = nombre_archivo.split(\"_chunk_\")[0]\n",
    "\tchunk_cero = f\"{base_name}_chunk_0.txt\"\n",
    "\n",
    "\t# Construir la ruta completa\n",
    "\truta_chunk_0 = os.path.join(carpeta_texto, chunk_cero)\n",
    "\n",
    "\t# Leer el contenido si existe\n",
    "\tif os.path.exists(ruta_chunk_0):\n",
    "\t\ttry:\n",
    "\t\t\twith open(ruta_chunk_0, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\t\tcontenido = f.read().strip()\n",
    "\t\t\t\treturn contenido\n",
    "\t\texcept Exception as e:\n",
    "\t\t\treturn \"\"\n",
    "\telse:\n",
    "\t\treturn \"\"\n",
    "\n",
    "\n",
    "# Cargar modelo liviano en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "def detectar_autor_ia(archivo: str) -> str:\n",
    "\t\"\"\"\n",
    "\t\tUsa spaCy para detectar entidades tipo PERSON en el texto.\n",
    "\t\"\"\"\n",
    "\ttexto = obtener_chunk_0(archivo)\n",
    "\tdoc = nlp(texto)\n",
    "\tpersonas = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "\tif personas:\n",
    "\t\treturn personas[0].title()\n",
    "\treturn \"Desconocido\"\n",
    "\n",
    "\n",
    "def rag_search(query: str, mode: str = \"semantic\", k: int = 4):\n",
    "\t\"\"\"\n",
    "\t\tRealiza una búsqueda semántica en la base vectorial local.\n",
    "\n",
    "\t\tParámetros:\n",
    "\t\t\tquery: consulta del usuario\n",
    "\t\t\tmode: 'fixed' (segmentación fija) | 'semantic' (segmentación semántica)\n",
    "\t\t\tk: cantidad de fragmentos a recuperar\n",
    "\n",
    "\t\tRetorna:\n",
    "\t\t\ttextos (List[str]), metadatos (List[dict])\n",
    "\t\"\"\"\n",
    "\tstore = chroma_sem if mode == \"semantic\" else chroma_fixed\n",
    "\n",
    "\tprefixed_query = f\"query: {query.strip()}\"\n",
    "\tresults = store.similarity_search(prefixed_query, k=k)\n",
    "\t\n",
    "\ttextos = [r.page_content for r in results]\n",
    "\tmetadatos = [r.metadata for r in results]\n",
    "\n",
    "\treturn textos, metadatos\n",
    "\n",
    "\n",
    "def rag_tool(query: str, mode=\"semantic\", k=4) -> str:\n",
    "\t\"\"\"\n",
    "\t\tTool de recuperación local: devuelve fragmentos relevantes.\n",
    "\t\"\"\"\n",
    "\ttextos, metas = rag_search(query, mode=mode, k=k)\n",
    "\tbloques = []\n",
    "\n",
    "\tfor t, m in zip(textos, metas):\n",
    "\n",
    "\t\tsrc = os.path.basename(m.get(\"ruta_fragmento\", \"\"))\n",
    "\t\tfrag_id = m.get(\"fragmento_id\", \"\")\n",
    "\t\tbloques.append(f\"\\n\")\n",
    "\n",
    "\t\tautor = detectar_autor_ia(src)\n",
    "\t\tbloques.append(f\"[Fuente: {src} | ID: {frag_id} | Autor: {autor}]\\n{t}\\n\")\n",
    "\t\t\n",
    "\treturn \"\".join(bloques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc37863",
   "metadata": {},
   "source": [
    "#### Creación de la Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eaabbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Tools creadas.\n"
     ]
    }
   ],
   "source": [
    "RAG_Fixed_Tool = Tool(\n",
    "    name=f\"{agent_name} RAG Tool - Fixed\",\n",
    "    description=\"Busca en la base vectorial de los apuntes con segmentación fija.\",\n",
    "    func=lambda q: rag_tool(q, mode=\"fixed\"),\n",
    ")\n",
    "\n",
    "RAG_Sem_Tool = Tool(\n",
    "    name=f\"{agent_name} RAG Tool - Semantic\",\n",
    "    description=\"Busca en la base vectorial de los apuntes con segmentación semántica.\",\n",
    "    func=lambda q: rag_tool(q, mode=\"semantic\"),\n",
    ")\n",
    "\n",
    "print(\"RAG Tools creadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace86f5",
   "metadata": {},
   "source": [
    "### Herramienta WebSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7c62a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_tool(query: str, serpapi_key: str, max_results: int = 10) -> str:\n",
    "    \"\"\"\n",
    "\t\tBúsqueda web avanzada con SerpAPI.\n",
    "\t\tRecibe la API key como parámetro (sin depender de variables de entorno).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search = SerpAPIWrapper(serpapi_api_key=serpapi_key)\n",
    "        results = search.results(query)\n",
    "\n",
    "        formatted = []\n",
    "        for r in results.get(\"organic_results\", [])[:max_results]:\n",
    "            title = r.get(\"title\", \"Sin título\")\n",
    "            snippet = r.get(\"snippet\", \"\").strip()\n",
    "            link = r.get(\"link\", \"\")\n",
    "            source = r.get(\"source\", \"Desconocido\")\n",
    "\n",
    "            formatted.append(f\"[{title}]({link}) — {snippet} _(Fuente: {source})_\")\n",
    "\n",
    "        if not formatted:\n",
    "            return f\"No se encontraron resultados relevantes para '{query}'.\"\n",
    "\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[Error en búsqueda web] {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a50c4",
   "metadata": {},
   "source": [
    "#### Creación de la Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4acb3ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSearch Tool creada.\n"
     ]
    }
   ],
   "source": [
    "SERPAPI_KEY = \"dde1f999814b7ea544a7a9c6a64718f74423912d64b1182f2381557ed384feb8\"\n",
    "\n",
    "WebSearch_Tool = Tool(\n",
    "    name=\"WebSearch_Tool\",\n",
    "    description=\"Realiza una búsqueda web con SerpAPI y devuelve enlaces relevantes con contexto.\",\n",
    "    func=lambda q: web_search_tool(q, serpapi_key=SERPAPI_KEY)\n",
    ")\n",
    "\n",
    "print(\"WebSearch Tool creada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fcdf7",
   "metadata": {},
   "source": [
    "## 5. Perfil, Orquestación y Memoria del Agente LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61ab3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMemory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, message):\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "# === Memoria ===\n",
    "memory = SimpleMemory()\n",
    "\n",
    "# === Perfil del agente ===\n",
    "AGENT_PERSONA = f\"\"\"\n",
    "Eres {agent_name}, un asistente académico especializado en el curso de Inteligencia Artificial del Instituto Tecnológico de Costa Rica.\n",
    "\n",
    "Rol:\n",
    "- Actúas como tutor técnico y conceptual, capaz de responder preguntas relacionadas con los temas de IA vistos en clase (búsqueda, lógica, planificación, aprendizaje supervisado y no supervisado, redes neuronales, etc.).\n",
    "- Siempre que cites información, indícalo claramente entre corchetes, con el nombre del archivo o la fuente.\n",
    "\n",
    "Estilo de comunicación:\n",
    "- Claro, formal y pedagógico.\n",
    "- Explica los conceptos con ejemplos breves cuando sea necesario.\n",
    "- Evita repeticiones o información innecesaria.\n",
    "- Siempre referencia tanto el nombre de las fuentes que te den en el contexto, como las referencias al buscar en la web.\n",
    "\n",
    "Restricciones:\n",
    "- No digas que no puedes hacer búsquedas web ni menciones herramientas.\n",
    "- No repitas la instrucción de cómo obtuviste la información, simplemente usa el contexto disponible.\n",
    "- No inventes información. Si no hay datos suficientes en el contexto, indícalo con claridad y sugiere una búsqueda web.\n",
    "\n",
    "Instrucción general:\n",
    "- Utiliza exclusivamente la información incluida en el bloque [Contexto obtenido de ...]. Si la pregunta no tiene nada que ver con el contexto, entonces no hagas referencia al contexto.\n",
    "- Si el contexto contiene material de apuntes o resultados de búsqueda web, intégralos directamente en tu respuesta.\n",
    "- Mantén un tono académico, pero conciso.\n",
    "\"\"\"\n",
    "\n",
    "# === LLM ===\n",
    "llm = OllamaLLM(model=\"llama3.2\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6999e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preguntar al Agente ===\n",
    "def responder_usuario(\n",
    "\t\tpregunta: str,\n",
    "\t\tk: int = 5,\n",
    "\t\tverbose: bool = False,\n",
    "\t\tsegmentacion=\"semantic\",\n",
    "\t\ttools=None,\n",
    "\t\tagent_profile: str = \"\",\n",
    "\t\tllm=None,\n",
    "\t\tmemory=None\n",
    "\t):\t\n",
    "\t\"\"\"\n",
    "\t\tDecide automáticamente si usar RAG o WebSearch\n",
    "\t\tsegún el tipo de pregunta.\n",
    "\t\"\"\"\n",
    "\tpregunta_lower = pregunta.lower()\n",
    "\n",
    "\t# --- Selección automática de herramienta ---\n",
    "\tif any(palabra in pregunta_lower for palabra in [\"web\", \"internet\", \"buscar en internet\", \"navegar\", \"búsqueda web\", \"investiga\"]):\n",
    "\t\tcontexto = tools[2].run(pregunta)\n",
    "\t\tfuente = \"búsqueda web\"\n",
    "\t\tinstruccion_extra = \"Este contexto te lo dio el tool de la búsqueda web. Usa la información del contexto que se te dio, es el resultado de la búsqueda web para responder, tú no tienes que buscar nada extra. Referencia las fuentes y escribe los enlaces al final de tu respuesta.\"\n",
    "\n",
    "\telif segmentacion == \"fixed\":\n",
    "\t\tcontexto = tools[1].run(pregunta)\n",
    "\t\tfuente = \"apuntes (segmentación fija)\"\n",
    "\t\tinstruccion_extra = \"Este contexto te lo dio el tool del RAG fijo que obtiene información de los apuntes. Responde solo usando la información de los apuntes. Referencia las fuentes al final de tu respuesta. Si el contexto dado no tiene nada que ver con la pregunta, no referencias, solo habla normalmente.\"\n",
    "\n",
    "\telif segmentacion == \"semantic\":\n",
    "\t\tcontexto = tools[0].run(pregunta)\n",
    "\t\tfuente = \"apuntes (segmentación semántica)\"\n",
    "\t\tinstruccion_extra = \"Este contexto te lo dio el tool del RAG semántico que obtiene información de los apuntes. Responde solo usando la información de los apuntes. Referencia las fuentes al final de tu respuesta. Si el contexto dado no tiene nada que ver con la pregunta, no referencias, solo habla normalmente.\"\n",
    "\n",
    "\telse:\n",
    "\t\tcontexto = \"\"\n",
    "\t\tfuente = \"\"\n",
    "\t\tinstruccion_extra = \"\"\n",
    "\n",
    "\t# --- Carga del historial de la conversación ---\n",
    "\thistorial = \"\"\n",
    "\tfor m in memory.messages[-k:]:  \t\t# solo los últimos k turnos\n",
    "\t\tif isinstance(m, HumanMessage):\n",
    "\t\t\thistorial += f\"Usuario: {m.content}\\n\"\n",
    "\t\telif isinstance(m, AIMessage):\n",
    "\t\t\thistorial += f\"Asistente: {m.content}\\n\"\n",
    "\n",
    "\t# --- Construcción del prompt completo ---\n",
    "\tprompt = f\"\"\"\n",
    "{agent_profile}\n",
    "\n",
    "[Historial reciente de conversación]\n",
    "{historial}\n",
    "\n",
    "[Contexto obtenido de {fuente}]\n",
    "{contexto}\n",
    "\n",
    "{instruccion_extra}\n",
    "\n",
    "Pregunta del usuario: {pregunta}\n",
    "Tu respuesta:\n",
    "\t\"\"\"\n",
    "\n",
    "\tif (verbose):\n",
    "\t\tprint(f\"{prompt}\")\n",
    "\n",
    "\t# --- Invocar modelo local de Ollama ---\n",
    "\trespuesta = llm.invoke(prompt)\n",
    "\n",
    "\t# --- Guardar Memoria ---\n",
    "\tmemory.add_message(HumanMessage(content=pregunta))\n",
    "\tmemory.add_message(AIMessage(content=respuesta))\n",
    "\n",
    "\treturn respuesta.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9683db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Ejemplo de uso ===\n",
    "# tools = [RAG_Sem_Tool, RAG_Fixed_Tool, WebSearch_Tool]\n",
    "# print(\"------------------------------------------------------------\")\n",
    "# pregunta = \"Cuales son las fórmulas de precision y de accuracy?\"\n",
    "# respuesta = responder_usuario(pregunta, verbose=True, segmentacion=\"semantic\", tools=tools, agent_profile = AGENT_PERSONA, llm=llm, memory=memory)\n",
    "# print(\"Pregunta:\\n\", pregunta)\n",
    "# print(\"Respuesta del agente:\\n\", respuesta)\n",
    "\n",
    "# print(\"------------------------------------------------------------\")\n",
    "# pregunta = \"Haz una búsqueda web sobre las matrices de confusion\"\n",
    "# respuesta = responder_usuario(pregunta, verbose=False, segmentacion=\"semantic\", tools=tools, agent_profile = AGENT_PERSONA, llm=llm, memory=memory)\n",
    "# print(\"Pregunta:\\n\", pregunta)\n",
    "# print(\"Respuesta del agente:\\n\", respuesta)\n",
    "\n",
    "# print(\"------------------------------------------------------------\")\n",
    "# pregunta = \"Cual fue la primera fórmula que te pregunté?\"\n",
    "# respuesta = responder_usuario(pregunta, verbose=True, segmentacion=\"semantic\", tools=tools, agent_profile = AGENT_PERSONA, llm=llm, memory=memory)\n",
    "# print(\"Pregunta:\\n\", pregunta)\n",
    "# print(\"Respuesta del agente:\\n\", respuesta)\n",
    "\n",
    "# print(\"------------------------------------------------------------\")\n",
    "# pregunta = \"Haz una búsqueda web sobre el perceptrón\"\n",
    "# respuesta = responder_usuario(pregunta, verbose=False, segmentacion=\"semantic\", tools=tools, agent_profile = AGENT_PERSONA, llm=llm, memory=memory)\n",
    "# print(\"Pregunta:\\n\", pregunta)\n",
    "# print(\"Respuesta del agente:\\n\", respuesta)\n",
    "\n",
    "# print(\"------------------------------------------------------------\")\n",
    "# pregunta = \"¿Cómo se calcula la derivada de la función de pérdida?\"\n",
    "# respuesta = responder_usuario(pregunta, verbose=True, segmentacion=\"semantic\", tools=tools, agent_profile = AGENT_PERSONA, llm=llm, memory=memory)\n",
    "# print(\"Pregunta:\\n\", pregunta)\n",
    "# print(\"Respuesta del agente:\\n\", respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504222a",
   "metadata": {},
   "source": [
    "## 6. Aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccf6be",
   "metadata": {},
   "source": [
    "Hay que exportar este notebook a un .py y correr en una terminal:\n",
    "```bash\n",
    "streamlit run notebook.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500f9b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:58:34.075 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.075 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando aplicación...\n",
      "Inicializando el agente por primera vez...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:58:34.710 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.712 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.712 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.712 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.714 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.714 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.714 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.714 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.715 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.715 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.716 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.716 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.717 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.718 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.718 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:34.719 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agente inicializado correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 18:58:35.856 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\lfben\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-11-05 18:58:35.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.868 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.869 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.871 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-05 18:58:35.871 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "def crear_interfaz_agente():\n",
    "\tst.set_page_config(page_title=\"Agente QWERTY\", layout=\"wide\")\n",
    "\n",
    "\t# === Inicialización persistente ===\n",
    "\tif \"agente_inicializado\" not in st.session_state:\n",
    "\t\tprint(\"Inicializando el agente por primera vez...\")\n",
    "\n",
    "\t\t# --- Crear objetos persistentes ---\n",
    "\t\tst.session_state.llm = OllamaLLM(model=\"llama3.2\", temperature=0.2)\n",
    "\t\tst.session_state.memory = SimpleMemory()\n",
    "\t\tst.session_state.tools = [RAG_Sem_Tool, RAG_Fixed_Tool, WebSearch_Tool]\n",
    "\t\tst.session_state.AGENT_PERSONA = AGENT_PERSONA\n",
    "\n",
    "\t\tst.session_state.mensajes = []\n",
    "\t\tst.session_state.agente_inicializado = True\n",
    "\t\tprint(\"Agente inicializado correctamente\")\n",
    "\n",
    "\t# === Interfaz de usuario ===\n",
    "\tcol1, col2 = st.columns([0.7, 0.3])\n",
    "\twith col1:\n",
    "\t\tst.title(\"QWERTY\")\n",
    "\t\tst.caption(\"Haz preguntas sobre los apuntes del curso de Inteligencia Artificial (LLaMA 3.2)\")\n",
    "\twith col2:\n",
    "\t\tsegmentacion = st.selectbox(\n",
    "\t\t\t\"Segmentación\",\n",
    "\t\t\t[\"semantic\", \"fixed\"],\n",
    "\t\t\tindex=0,\n",
    "\t\t\tformat_func=lambda x: \"Semántica\" if x == \"semantic\" else \"Fija\"\n",
    "\t\t)\n",
    "\t\tst.session_state.segmentacion = segmentacion\n",
    "\n",
    "\t# === Historial de conversación ===\n",
    "\tfor msg in st.session_state.mensajes:\n",
    "\t\tst.chat_message(msg[\"role\"]).markdown(msg[\"content\"])\n",
    "\n",
    "\t# === Entrada de usuario ===\n",
    "\tpregunta = st.chat_input(f\"Escribe tu pregunta ({segmentacion})...\")\n",
    "\n",
    "\tif pregunta:\n",
    "\t\tst.chat_message(\"user\").markdown(pregunta)\n",
    "\t\tst.session_state.mensajes.append({\"role\": \"user\", \"content\": pregunta})\n",
    "\n",
    "\t\twith st.spinner(\"Pensando...\"):\n",
    "\t\t\trespuesta = responder_usuario(\n",
    "\t\t\t\tpregunta,\n",
    "\t\t\t\tsegmentacion=segmentacion,\n",
    "\t\t\t\tverbose=True,\n",
    "\t\t\t\tk=10,\n",
    "\t\t\t\tllm=st.session_state.llm,\n",
    "\t\t\t\tmemory=st.session_state.memory,\n",
    "\t\t\t\ttools=st.session_state.tools,\n",
    "\t\t\t\tagent_profile=st.session_state.AGENT_PERSONA\n",
    "\t\t\t)\n",
    "\n",
    "\t\tst.chat_message(\"assistant\").markdown(respuesta)\n",
    "\t\tst.session_state.mensajes.append({\"role\": \"assistant\", \"content\": respuesta})\n",
    "\n",
    "\n",
    "# === Ejecución directa ===\n",
    "if __name__ == \"__main__\":\n",
    "\tprint(\"Iniciando aplicación...\")\n",
    "\tcrear_interfaz_agente()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
